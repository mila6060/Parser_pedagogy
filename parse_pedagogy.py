import requests
from bs4 import BeautifulSoup
import json
from urllib.parse import urljoin

def parse_pedagogy_articles():
    """
    –ü–∞—Ä—Å–∏—Ç –∫–∞—Ä—Ç–æ—á–∫–∏ —Å—Ç–∞—Ç–µ–π —Å —Å–∞–π—Ç–∞ pedsovet.org
    """
    base_url = "https://pedsovet.org"
    
    try:
        print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É...")
        response = requests.get(base_url)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        articles_data = []
        
        # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ —Å—Ç–∞—Ç–µ–π
        print("üîç –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ —Å—Ç–∞—Ç–µ–π...")
        cards = soup.find_all(class_=lambda x: x and any(
            word in x for word in ['news', 'article', 'post', 'item', 'card']))
        
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –∫–∞—Ä—Ç–æ—á–µ–∫: {len(cards)}")
        
        for i, card in enumerate(cards, 1):
            try:
                # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                title_elem = card.find(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']) or card.find('a')
                link_elem = card.find('a')
                
                if title_elem and link_elem and link_elem.get('href'):
                    title = title_elem.get_text(strip=True)
                    link = link_elem.get('href')
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–æ—Ü—Å–µ—Ç–∏
                    if len(title) > 10 and not any(social in link for social in ['facebook', 'twitter', 'vk']):
                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏
                        if link.startswith('/'):
                            link = urljoin(base_url, link)
                        
                        article_data = {
                            'id': i,
                            'title': title,
                            'url': link
                        }
                        articles_data.append(article_data)
                        print(f"‚úÖ –ö–∞—Ä—Ç–æ—á–∫–∞ {i}: {title[:60]}...")
                        
            except Exception as e:
                continue
        
        return articles_data
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return []

def save_to_json(data, filename='articles.json'):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ JSON —Ñ–∞–π–ª"""
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")

def main():
    print("üéì –ü–ê–†–°–ï–† –°–¢–ê–¢–ï–ô –ü–û –ü–ï–î–ê–ì–û–ì–ò–ö–ï")
    print("=" * 40)
    
    articles = parse_pedagogy_articles()
    
    if articles:
        print(f"\nüéØ –£—Å–ø–µ—à–Ω–æ —Å–æ–±—Ä–∞–Ω–æ: {len(articles)} —Å—Ç–∞—Ç–µ–π")
        
        # –í—ã–≤–æ–¥–∏–º –≤ –∫–æ–Ω—Å–æ–ª—å
        for article in articles:
            print(f"\nüìñ {article['title']}")
            print(f"üîó {article['url']}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
        save_to_json(articles)
    else:
        print("üòî –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ")

if __name__ == "__main__":
    main()